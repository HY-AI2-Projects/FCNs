# FCNs
FCN is a model that emerged in the field of computer vision in 2015, a time when there were not many artificial intelligence models. While there were classification models like VGGnet and GoogLeNet, and an object detection model called the R-CNN model, there was no specific solution for segmentation problems in computer vision tasks at that time. Although traditional CNN models could be used for segmentation, they were primarily classification-based models. This meant they analyzed the entire image by finding and abstracting global features. Object detection models were not widely applicable as they required dedicated pattern recognition filters. This text aims to explain the FCN model, which allows predictions for each pixel. It's worth noting that conventional segmentation methods used patchwise and pixelwise approaches, but these methods were computationally expensive and involved significant pre and post-processing due to issues like superpixels.

Let's define three key features of the FCN model:

Firstly, the significance of the FCN model lies in being a preprocessing-free image segmentation model. Traditional CNN models go through convolutional layers, pooling layers, and end with a fully connected layer. In contrast, FCN does not have a fully connected layer. Typical CNN models extract local features of the image through convolutional and pooling layers and abstract this process to find class information in the output layer through a fully connected layer. For example, the VGG16 model has an output layer composed of 1000 nodes for classifying 1000 classes. However, the problem with this structure is that the weights of the fully connected layer are fixed, causing the size of the feature map from the preceding layer and consequently the input image size to be fixed. FCN replaces this fully connected layer with a convolutional layer, a technique known as convolutionalization. As a result, there are no structural constraints on the output layer, eliminating the limitation on input size. FCN is a technology in the segmentation field, requiring both the classification ability of CNN and localization information about the object's position. Conventional CNN models, focusing on extracting local features, lose information about the position since the process abstracts away the detailed spatial information of the entire image. This problem arises in the fully connected layer where all pixels are connected in a line. FCN addresses this by removing the fully connected layer, applying convolutionalization, and representing the information for each pixel in heatmap format. The convolutionalization method involves applying multiple 1x1 filters to maintain the number of weights according to the number of nodes in the original fully connected layer. In the case of VGG16, which has 4096 nodes in the fully connected layer, this method applies 4096 1x1 filters.

To summarize, the significance of FCN lies in performing image segmentation without preprocessing by utilizing convolutionalization. Secondly, FCN employs specific techniques for image segmentation. As seen in the previous content, compressed heatmaps are extracted by convolutionalization to predict pixel-wise segmentation. However, a special technique is needed for the final pixel-wise prediction from the compressed image. This technique involves using an inverse filtering method to enlarge the compressed image. FCN utilizes shift and stitch, bilinear interpolation, and the deconvolution method. The deconvolution method restores the size of the image by using the reverse operation of convolution, allowing pixel-wise prediction. Thirdly, the structure of FCN can be viewed as an encoder-decoder structure. In the encoder structure, the same process of compressing/abstracting images for classification is performed, and the feature map is compressed 1/32 times through 5 convolutional-pooling layers. The encoder obtains meaningful information about the entire image, which is global information, depending on the goal of CNN. The decoder then enhances the compressed feature map.

The most basic FCN-32 method involves 32x enlargement through deconvolution, but the image quality deteriorates. To address this, the FCN-16 model was introduced. This model enlarges by 2 times, then combines 4 times the feature map of the pooling layer 4, and again enlarges 16 times. It brings in parts of the original feature map to improve the image quality. The FCN-8 model, applying the same method, enlarges by 2 times, combines 4 times the feature map of pooling layer 4, enlarges by 2 times, combines 3 times the feature map of pooling layer 3, and then enlarges 8 times. As a result, the decoder can obtain detailed image information from the shallow layer before encoding, and when looking at the performance metrics in related papers, the pixel accuracy between the actual image and the predicted image increases. In summary, the structure of FCN can be summarized as a model that restores information that could have been lost during encoding through skip architecture, combining information from the previous feature map, and mixing the entire image information obtained through encoding with the detailed image information obtained through decoding to perform segmentation. Ultimately, the FCN model created a structure that eliminates the need for preprocessing through convolutionalization, allows input regardless of image size, preserves image position information, enables pixel-wise segmentation through deconvolution, and uses skip architecture for more refined segmentation. This model achieved a 20% improvement in Mean IU over the CK model in the Pascal VOC 2012 segmentation test.
